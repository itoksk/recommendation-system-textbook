# Chapter 5: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªè¨­è¨ˆ ğŸš€

## ğŸ“š ã“ã®ç« ã§å­¦ã¶ã“ã¨

- å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆåŸå‰‡
- Product Mixerãƒ‘ã‚¿ãƒ¼ãƒ³
- åˆ†æ•£å‡¦ç†ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã®å®Ÿè£…

## ğŸ—ï¸ Twitterã‚¹ã‚±ãƒ¼ãƒ«ã®èª²é¡Œ

### æ•°å­—ã§è¦‹ã‚‹Twitterã®ã‚¹ã‚±ãƒ¼ãƒ«

```python
class TwitterScale:
    """TwitterãŒæ‰±ã†ãƒ‡ãƒ¼ã‚¿è¦æ¨¡"""
    
    STATS = {
        'daily_tweets': 500_000_000,      # 1æ—¥5å„„ãƒ„ã‚¤ãƒ¼ãƒˆ
        'active_users': 330_000_000,      # æœˆé–“ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼
        'queries_per_second': 300_000,    # ç§’é–“30ä¸‡ã‚¯ã‚¨ãƒª
        'timeline_loads': 200_000_000,    # 1æ—¥2å„„å›ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¡¨ç¤º
        'simclusters': 145_000,           # 14.5ä¸‡ã‚¯ãƒ©ã‚¹ã‚¿
        'features_per_tweet': 6_000,      # ãƒ„ã‚¤ãƒ¼ãƒˆã‚ãŸã‚Š6000ç‰¹å¾´é‡
    }
    
    @classmethod
    def calculate_requirements(cls):
        """å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã‚’è¨ˆç®—"""
        # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®å‡¦ç†
        candidates_per_request = 100_000   # å€™è£œãƒ„ã‚¤ãƒ¼ãƒˆ
        light_ranker_time = 0.1           # ms/ãƒ„ã‚¤ãƒ¼ãƒˆ
        heavy_ranker_time = 10            # ms/ãƒ„ã‚¤ãƒ¼ãƒˆ
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶
        max_latency = 100  # ms
        
        # å¿…è¦ãªä¸¦åˆ—åº¦
        parallel_factor = (candidates_per_request * light_ranker_time) / max_latency
        
        print(f"å¿…è¦ãªä¸¦åˆ—åº¦: {parallel_factor:.0f}å€")
        print(f"1ç§’ã‚ãŸã‚Šã®å‡¦ç†é‡: {cls.STATS['queries_per_second'] * candidates_per_request:,}")
```

## ğŸ­ Product Mixerãƒ‘ã‚¿ãƒ¼ãƒ³

### Twitterã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹è¨­è¨ˆ

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import asyncio

class Component(ABC):
    """åŸºæœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    @abstractmethod
    async def execute(self, request: Dict, state: Dict) -> Dict:
        pass

class Pipeline:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼šã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ„ã¿åˆã‚ã›ã‚‹"""
    
    def __init__(self, components: List[Component]):
        self.components = components
        
    async def execute(self, request: Dict) -> Dict:
        state = {'request': request}
        
        for component in self.components:
            try:
                state = await component.execute(request, state)
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                state['error'] = str(e)
                break
                
        return state

class ProductMixer:
    """
    Product Mixer: Twitterã®ã‚³ã‚¢è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³
    - å†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    - å®£è¨€çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
    - è‡ªå‹•çš„ãªç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°
    """
    
    def __init__(self):
        self.pipelines = {}
        
    def register_pipeline(self, name: str, pipeline: Pipeline):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç™»éŒ²"""
        self.pipelines[name] = pipeline
        
    async def execute(self, pipeline_name: str, request: Dict) -> Dict:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        pipeline = self.pipelines.get(pipeline_name)
        if not pipeline:
            raise ValueError(f"Pipeline {pipeline_name} not found")
            
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        start_time = asyncio.get_event_loop().time()
        
        result = await pipeline.execute(request)
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨˜éŒ²
        latency = asyncio.get_event_loop().time() - start_time
        result['latency_ms'] = latency * 1000
        
        return result

# å®Ÿéš›ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®Ÿè£…ä¾‹

class CandidateSourceComponent(Component):
    """å€™è£œç”Ÿæˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    def __init__(self, sources: List[str]):
        self.sources = sources
        
    async def execute(self, request: Dict, state: Dict) -> Dict:
        # ä¸¦åˆ—ã§å€™è£œã‚’å–å¾—
        tasks = []
        for source in self.sources:
            task = self.fetch_candidates(source, request['user_id'])
            tasks.append(task)
            
        candidates_lists = await asyncio.gather(*tasks)
        
        # çµæœã‚’ãƒãƒ¼ã‚¸
        all_candidates = []
        for candidates in candidates_lists:
            all_candidates.extend(candidates)
            
        state['candidates'] = all_candidates
        return state
    
    async def fetch_candidates(self, source: str, user_id: str):
        """å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å€™è£œã‚’å–å¾—"""
        # å®Ÿéš›ã¯RPCã‚„DBã‚¢ã‚¯ã‚»ã‚¹
        await asyncio.sleep(0.01)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return [f"{source}_candidate_{i}" for i in range(100)]

class FeatureHydrationComponent(Component):
    """ç‰¹å¾´é‡ä»˜ä¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    async def execute(self, request: Dict, state: Dict) -> Dict:
        candidates = state['candidates']
        
        # ãƒãƒƒãƒã§ç‰¹å¾´é‡ã‚’å–å¾—
        features = await self.batch_fetch_features(candidates)
        
        # å€™è£œã«ç‰¹å¾´é‡ã‚’ä»˜ä¸
        for candidate, feature in zip(candidates, features):
            candidate['features'] = feature
            
        return state
    
    async def batch_fetch_features(self, candidates):
        """ãƒãƒƒãƒã§ç‰¹å¾´é‡å–å¾—ï¼ˆåŠ¹ç‡åŒ–ï¼‰"""
        # å®Ÿéš›ã¯ç‰¹å¾´é‡ã‚¹ãƒˆã‚¢ã‹ã‚‰å–å¾—
        await asyncio.sleep(0.02)
        return [{'feature': i} for i in range(len(candidates))]

class ScoringComponent(Component):
    """ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    def __init__(self, model):
        self.model = model
        
    async def execute(self, request: Dict, state: Dict) -> Dict:
        candidates = state['candidates']
        
        # ãƒãƒƒãƒäºˆæ¸¬
        scores = await self.model.predict_batch(candidates)
        
        # ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸
        for candidate, score in zip(candidates, scores):
            candidate['score'] = score
            
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        state['candidates'] = sorted(
            candidates, 
            key=lambda x: x['score'], 
            reverse=True
        )
        
        return state

class FilteringComponent(Component):
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    async def execute(self, request: Dict, state: Dict) -> Dict:
        candidates = state['candidates']
        user = request['user']
        
        filtered = []
        seen_authors = set()
        
        for candidate in candidates:
            # å¤šæ§˜æ€§ãƒ•ã‚£ãƒ«ã‚¿
            if candidate['author_id'] in seen_authors:
                continue
                
            # ãƒ–ãƒ­ãƒƒã‚¯/ãƒŸãƒ¥ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿
            if candidate['author_id'] in user['blocked_users']:
                continue
                
            filtered.append(candidate)
            seen_authors.add(candidate['author_id'])
            
            if len(filtered) >= 20:
                break
                
        state['candidates'] = filtered
        return state

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰

def build_timeline_pipeline():
    """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰"""
    
    pipeline = Pipeline([
        # 1. å€™è£œç”Ÿæˆ
        CandidateSourceComponent(sources=[
            'in_network',      # ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦ã„ã‚‹äºº
            'out_of_network',  # ãƒ•ã‚©ãƒ­ãƒ¼å¤–
            'trending',        # ãƒˆãƒ¬ãƒ³ãƒ‰
        ]),
        
        # 2. ç‰¹å¾´é‡ä»˜ä¸
        FeatureHydrationComponent(),
        
        # 3. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        ScoringComponent(model=LightRanker()),
        
        # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        FilteringComponent(),
        
        # 5. è©³ç´°ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆä¸Šä½ã®ã¿ï¼‰
        ScoringComponent(model=HeavyRanker()),
        
        # 6. æœ€çµ‚èª¿æ•´
        FilteringComponent(),
    ])
    
    return pipeline
```

## âš¡ åˆ†æ•£å‡¦ç†ã®å®Ÿè£…

### ä¸¦åˆ—å‡¦ç†ã¨ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```python
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
import grpc

class DistributedRanker:
    """åˆ†æ•£ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, n_shards=100):
        self.n_shards = n_shards
        self.executor = ThreadPoolExecutor(max_workers=50)
        self.shard_clients = self._init_shard_clients()
        
    def _init_shard_clients(self):
        """å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¸ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        clients = {}
        for shard_id in range(self.n_shards):
            # gRPCã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
            channel = grpc.insecure_channel(f'shard-{shard_id}:50051')
            clients[shard_id] = RankerStub(channel)
        return clients
    
    def get_shard(self, item_id: str) -> int:
        """ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’æ±ºå®š"""
        hash_value = hashlib.md5(item_id.encode()).hexdigest()
        return int(hash_value, 16) % self.n_shards
    
    async def rank_distributed(self, candidates: List[str], user_context: Dict):
        """åˆ†æ•£ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        
        # 1. å€™è£œã‚’ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨ã«åˆ†å‰²
        sharded_candidates = defaultdict(list)
        for candidate in candidates:
            shard_id = self.get_shard(candidate)
            sharded_candidates[shard_id].append(candidate)
            
        # 2. ä¸¦åˆ—ã§RPC
        futures = []
        for shard_id, shard_candidates in sharded_candidates.items():
            future = self.executor.submit(
                self._rank_on_shard,
                shard_id,
                shard_candidates,
                user_context
            )
            futures.append(future)
            
        # 3. çµæœã‚’åé›†
        all_results = []
        for future in as_completed(futures):
            results = future.result()
            all_results.extend(results)
            
        # 4. ãƒãƒ¼ã‚¸ã—ã¦ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        return all_results[:100]
    
    def _rank_on_shard(self, shard_id: int, candidates: List, context: Dict):
        """ç‰¹å®šã®ã‚·ãƒ£ãƒ¼ãƒ‰ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        client = self.shard_clients[shard_id]
        
        request = RankRequest(
            candidates=candidates,
            user_context=context
        )
        
        response = client.Rank(request)
        return response.results

class MapReduceRanker:
    """MapReduceã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
    
    def map_phase(self, chunk: List, user_context: Dict) -> List:
        """Map: å„ãƒãƒ£ãƒ³ã‚¯ã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        local_ranker = LightRanker()
        scored = []
        
        for item in chunk:
            score = local_ranker.score(item, user_context)
            scored.append((item, score))
            
        # ãƒ­ãƒ¼ã‚«ãƒ«ã§Top-K
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:100]
    
    def reduce_phase(self, mapped_results: List[List]) -> List:
        """Reduce: çµæœã‚’ãƒãƒ¼ã‚¸"""
        all_results = []
        for chunk_results in mapped_results:
            all_results.extend(chunk_results)
            
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x[1], reverse=True)
        return all_results[:20]
    
    def rank(self, candidates: List, user_context: Dict, chunk_size: int = 1000):
        """MapReduceã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [
            candidates[i:i+chunk_size] 
            for i in range(0, len(candidates), chunk_size)
        ]
        
        # Map phaseï¼ˆä¸¦åˆ—ï¼‰
        with ThreadPoolExecutor() as executor:
            mapped = list(executor.map(
                lambda chunk: self.map_phase(chunk, user_context),
                chunks
            ))
            
        # Reduce phase
        final_results = self.reduce_phase(mapped)
        
        return final_results
```

## ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

### å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 

```python
import pickle
from functools import lru_cache
import memcache

class MultiLayerCache:
    """
    å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    L1: ãƒ—ãƒ­ã‚»ã‚¹å†…ãƒ¡ãƒ¢ãƒªï¼ˆæœ€é€Ÿï¼‰
    L2: Redis/Memcachedï¼ˆå…±æœ‰ï¼‰
    L3: ãƒ‡ã‚£ã‚¹ã‚¯/DBï¼ˆæ°¸ç¶šï¼‰
    """
    
    def __init__(self):
        self.l1_cache = {}  # ãƒ—ãƒ­ã‚»ã‚¹å†…
        self.l2_cache = memcache.Client(['localhost:11211'])  # Memcached
        self.l3_cache = DiskCache()  # ãƒ‡ã‚£ã‚¹ã‚¯
        
        # TTLè¨­å®š
        self.ttl_config = {
            'user_timeline': 60,      # 1åˆ†
            'tweet_features': 300,    # 5åˆ†
            'user_embeddings': 3600,  # 1æ™‚é–“
            'popular_tweets': 30,     # 30ç§’
        }
        
    def get(self, key: str, cache_type: str = 'default'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        
        # L1ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é€Ÿï¼‰
        if key in self.l1_cache:
            return self.l1_cache[key]
            
        # L2ãƒã‚§ãƒƒã‚¯
        value = self.l2_cache.get(key)
        if value:
            # L1ã«æ˜‡æ ¼
            self.l1_cache[key] = value
            return pickle.loads(value)
            
        # L3ãƒã‚§ãƒƒã‚¯
        value = self.l3_cache.get(key)
        if value:
            # L1, L2ã«æ˜‡æ ¼
            self.l2_cache.set(key, pickle.dumps(value), 
                            time=self.ttl_config.get(cache_type, 60))
            self.l1_cache[key] = value
            return value
            
        return None
    
    def set(self, key: str, value: Any, cache_type: str = 'default'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        ttl = self.ttl_config.get(cache_type, 60)
        
        # å…¨å±¤ã«ä¿å­˜
        self.l1_cache[key] = value
        self.l2_cache.set(key, pickle.dumps(value), time=ttl)
        self.l3_cache.set(key, value)
        
    def invalidate(self, key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        if key in self.l1_cache:
            del self.l1_cache[key]
        self.l2_cache.delete(key)
        self.l3_cache.delete(key)

class SmartCaching:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°"""
    
    def __init__(self):
        self.cache = MultiLayerCache()
        self.hit_rate = defaultdict(lambda: {'hits': 0, 'misses': 0})
        
    def get_with_fallback(self, key: str, fetch_func, cache_type: str = 'default'):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        value = self.cache.get(key, cache_type)
        
        if value is not None:
            self.hit_rate[cache_type]['hits'] += 1
            return value
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹
        self.hit_rate[cache_type]['misses'] += 1
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        value = fetch_func()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if value is not None:
            self.cache.set(key, value, cache_type)
            
        return value
    
    def precompute_popular(self):
        """äººæ°—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’äº‹å‰è¨ˆç®—"""
        # äººæ°—ãƒ„ã‚¤ãƒ¼ãƒˆã‚’äº‹å‰è¨ˆç®—
        popular_tweets = self.compute_popular_tweets()
        self.cache.set('popular_tweets', popular_tweets, 'popular_tweets')
        
        # äººæ°—ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’äº‹å‰è¨ˆç®—
        for user_id in self.get_popular_users():
            timeline = self.compute_timeline(user_id)
            self.cache.set(f'timeline:{user_id}', timeline, 'user_timeline')
```

## ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
from kafka import KafkaConsumer, KafkaProducer
import json

class StreamProcessor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†"""
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'tweet_events',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        self.processors = {
            'new_tweet': self.process_new_tweet,
            'like': self.process_like,
            'retweet': self.process_retweet,
            'follow': self.process_follow,
        }
        
    def start(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’é–‹å§‹"""
        for message in self.consumer:
            event = message.value
            event_type = event['type']
            
            # é©åˆ‡ãªãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’å®Ÿè¡Œ
            if event_type in self.processors:
                self.processors[event_type](event)
                
    def process_new_tweet(self, event):
        """æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å‡¦ç†"""
        tweet = event['tweet']
        
        # 1. ç‰¹å¾´é‡ã‚’æŠ½å‡º
        features = self.extract_features(tweet)
        
        # 2. åˆæœŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        score = self.light_ranker.score(features)
        
        # 3. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        if score > THRESHOLD:
            self.add_to_index(tweet, score)
            
        # 4. SimClustersåŸ‹ã‚è¾¼ã¿ã‚’åˆæœŸåŒ–
        self.init_tweet_embedding(tweet)
        
        # 5. ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ ï¼‰
        self.fanout_to_followers(tweet)
        
    def fanout_to_followers(self, tweet):
        """ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã«ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆ"""
        author_id = tweet['author_id']
        followers = self.get_followers(author_id)
        
        # ãƒãƒƒãƒå‡¦ç†
        batch = []
        for follower_id in followers:
            update = {
                'user_id': follower_id,
                'tweet_id': tweet['id'],
                'timestamp': tweet['created_at']
            }
            batch.append(update)
            
            if len(batch) >= 100:
                self.producer.send('timeline_updates', batch)
                batch = []
                
        # æ®‹ã‚Šã‚’é€ä¿¡
        if batch:
            self.producer.send('timeline_updates', batch)

class WindowedAggregation:
    """æ™‚é–“çª“ã§ã®é›†è¨ˆ"""
    
    def __init__(self, window_size_seconds=60):
        self.window_size = window_size_seconds
        self.windows = defaultdict(lambda: defaultdict(int))
        
    def add_event(self, event):
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¿½åŠ """
        timestamp = event['timestamp']
        window_id = timestamp // self.window_size
        
        # è©²å½“ã™ã‚‹çª“ã«è¿½åŠ 
        self.windows[window_id][event['type']] += 1
        
        # å¤ã„çª“ã‚’å‰Šé™¤
        current_window = time.time() // self.window_size
        expired_windows = [w for w in self.windows if w < current_window - 10]
        for window in expired_windows:
            del self.windows[window]
            
    def get_aggregates(self, n_windows=5):
        """ç›´è¿‘nå€‹ã®çª“ã®é›†è¨ˆã‚’å–å¾—"""
        current_window = time.time() // self.window_size
        
        aggregates = defaultdict(int)
        for i in range(n_windows):
            window_id = current_window - i
            if window_id in self.windows:
                for event_type, count in self.windows[window_id].items():
                    aggregates[event_type] += count
                    
        return dict(aggregates)
```

## ğŸ¯ æ¼”ç¿’å•é¡Œ

### æ¼”ç¿’5-1: ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
```python
def create_custom_component():
    """
    Product Mixerç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½œæˆ
    ä¾‹ï¼šã‚¹ãƒ‘ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€è¨€èªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãªã©
    """
    # TODO: å®Ÿè£…
    pass
```

### æ¼”ç¿’5-2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã®æ”¹å–„
```python
def optimize_cache_strategy():
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’æ”¹å–„ã™ã‚‹æˆ¦ç•¥ã‚’å®Ÿè£…
    """
    # TODO: å®Ÿè£…
    pass
```

## ğŸ“š ã¾ã¨ã‚

- **Product Mixer**: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã§æŸ”è»Ÿãªè¨­è¨ˆ
- **åˆ†æ•£å‡¦ç†**: ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨MapReduceã§ä¸¦åˆ—åŒ–
- **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**: å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é«˜é€ŸåŒ–
- **ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®æ›´æ–°ã¨é›†è¨ˆ
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å‰æã¨ã—ãŸè¨­è¨ˆ

[â†’ Chapter 6: å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸](chapter06_final_project.md)