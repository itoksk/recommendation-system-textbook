# Chapter 3: å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æ¥µã‚ã‚‹ ğŸ¤

## ğŸ“š ã“ã®ç« ã§å­¦ã¶ã“ã¨

- å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®æ•°å­¦çš„åŸºç¤
- ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ vs ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹
- Twitterã®SimClustersã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³è§£
- å®Ÿè£…ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

## ğŸ¯ å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®æœ¬è³ª

### ãªãœã€Œå”èª¿ã€ãªã®ã‹ï¼Ÿ

å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆCollaborative Filteringï¼‰ã¯ã€**å¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œå”èª¿ã€ã•ã›ã¦æ¨è–¦ã‚’è¡Œã†**æ‰‹æ³•ã§ã™ã€‚

```python
# åŸºæœ¬çš„ãªè€ƒãˆæ–¹
def collaborative_filtering_concept():
    """
    ã€Œã‚ãªãŸã¨ä¼¼ãŸäººãŒå¥½ããªã‚‚ã®ã¯ã€ã‚ãªãŸã‚‚å¥½ãã‹ã‚‚ã—ã‚Œãªã„ã€
    """
    # Step 1: é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹
    similar_users = find_similar_users(target_user)
    
    # Step 2: é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’é›†ç´„
    recommendations = aggregate_preferences(similar_users)
    
    return recommendations
```

## ğŸ” ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
import numpy as np
from scipy.spatial.distance import cosine

class UserBasedCF:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    
    def __init__(self, min_common_items=2):
        self.min_common_items = min_common_items
        self.user_item_matrix = None
        self.user_similarities = None
        
    def fit(self, ratings_data):
        """
        ratings_data: [(user_id, item_id, rating), ...]
        """
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—ã‚¢ã‚¤ãƒ†ãƒ è¡Œåˆ—ã‚’ä½œæˆ
        self.user_item_matrix = self._create_matrix(ratings_data)
        
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’äº‹å‰è¨ˆç®—
        self.user_similarities = self._calculate_all_similarities()
        
    def _calculate_similarity(self, user1_vec, user2_vec):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        # ä¸¡æ–¹ãŒè©•ä¾¡ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã®ã¿ä½¿ç”¨
        mask = (user1_vec > 0) & (user2_vec > 0)
        
        if mask.sum() < self.min_common_items:
            return 0.0
            
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        return 1 - cosine(user1_vec[mask], user2_vec[mask])
    
    def predict(self, user_id, item_id):
        """è©•ä¾¡ã‚’äºˆæ¸¬"""
        user_idx = self.user_map[user_id]
        item_idx = self.item_map[item_id]
        
        # é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©•ä¾¡ã‚’é‡ã¿ä»˜ã‘å¹³å‡
        similarities = self.user_similarities[user_idx]
        ratings = self.user_item_matrix[:, item_idx]
        
        # è©•ä¾¡æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿
        mask = ratings > 0
        if not mask.any():
            return self.global_mean
            
        weighted_sum = np.sum(similarities[mask] * ratings[mask])
        weight_sum = np.sum(np.abs(similarities[mask]))
        
        if weight_sum == 0:
            return self.global_mean
            
        return weighted_sum / weight_sum
```

### ã‚¢ã‚¤ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
class ItemBasedCF:
    """ã‚¢ã‚¤ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰"""
    
    def __init__(self):
        self.item_similarities = None
        
    def fit(self, ratings_data):
        """ã‚¢ã‚¤ãƒ†ãƒ é–“ã®é¡ä¼¼åº¦ã‚’äº‹å‰è¨ˆç®—"""
        # ã‚¢ã‚¤ãƒ†ãƒ Ã—ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œåˆ—
        item_user_matrix = self._create_item_matrix(ratings_data)
        
        # ã‚¢ã‚¤ãƒ†ãƒ é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—
        self.item_similarities = self._calculate_item_similarities(item_user_matrix)
        
    def _calculate_item_similarities(self, matrix):
        """èª¿æ•´ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆAdjusted Cosine Similarityï¼‰"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®å¹³å‡ã‚’å¼•ãï¼ˆå€‹äººã®è©•ä¾¡å‚¾å‘ã‚’é™¤å»ï¼‰
        user_means = matrix.mean(axis=0)
        adjusted_matrix = matrix - user_means
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(adjusted_matrix)
        
        return similarities
    
    def recommend(self, user_history, n=10):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ã‹ã‚‰æ¨è–¦"""
        scores = {}
        
        for item_id, rating in user_history.items():
            # ã“ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨é¡ä¼¼ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¢ã™
            similar_items = self.get_similar_items(item_id)
            
            for similar_item, similarity in similar_items:
                if similar_item not in user_history:
                    scores[similar_item] = scores.get(similar_item, 0)
                    scores[similar_item] += rating * similarity
                    
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:n]
```

## ğŸ§® ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

### è¡Œåˆ—åˆ†è§£ï¼ˆMatrix Factorizationï¼‰

```python
class MatrixFactorization:
    """
    ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã«ã‚ˆã‚‹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    R â‰ˆ P Ã— Q^T
    """
    
    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.01):
        self.n_factors = n_factors
        self.lr = learning_rate
        self.reg = regularization
        
    def fit(self, ratings_matrix, epochs=100):
        """ç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼ˆSGDï¼‰ã§å­¦ç¿’"""
        m, n = ratings_matrix.shape
        
        # ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–
        self.P = np.random.normal(0, 0.1, (m, self.n_factors))
        self.Q = np.random.normal(0, 0.1, (n, self.n_factors))
        
        for epoch in range(epochs):
            for i in range(m):
                for j in range(n):
                    if ratings_matrix[i, j] > 0:
                        # èª¤å·®ã‚’è¨ˆç®—
                        prediction = self.P[i] @ self.Q[j]
                        error = ratings_matrix[i, j] - prediction
                        
                        # å‹¾é…æ›´æ–°
                        p_gradient = error * self.Q[j] - self.reg * self.P[i]
                        q_gradient = error * self.P[i] - self.reg * self.Q[j]
                        
                        self.P[i] += self.lr * p_gradient
                        self.Q[j] += self.lr * q_gradient
                        
    def predict(self, user_idx, item_idx):
        """äºˆæ¸¬è©•ä¾¡ã‚’è¨ˆç®—"""
        return self.P[user_idx] @ self.Q[item_idx]
```

## ğŸ¦ Twitterã®SimClustersæ·±æ˜ã‚Š

### SimClustersã®é©æ–°æ€§

```python
class TwitterSimClusters:
    """
    Twitterã®å®Ÿè£…ã«è¿‘ã„SimClustersã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    ãƒã‚¤ãƒ³ãƒˆï¼šã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨è§£é‡ˆå¯èƒ½æ€§ã®ä¸¡ç«‹
    """
    
    def __init__(self, n_clusters=145000):
        self.n_clusters = n_clusters
        
    def build_producer_similarity_graph(self, follow_graph):
        """
        ç”Ÿç”£è€…ã®é¡ä¼¼åº¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        é¡ä¼¼åº¦ = ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®é‡è¤‡åº¦
        """
        producer_followers = defaultdict(set)
        
        # å„ç”Ÿç”£è€…ã®ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã‚’åé›†
        for consumer, producers in follow_graph.items():
            for producer in producers:
                producer_followers[producer].add(consumer)
                
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarity_edges = []
        producers = list(producer_followers.keys())
        
        for i, p1 in enumerate(producers):
            for p2 in producers[i+1:]:
                followers1 = producer_followers[p1]
                followers2 = producer_followers[p2]
                
                # Jaccardä¿‚æ•°
                intersection = len(followers1 & followers2)
                union = len(followers1 | followers2)
                
                if union > 0:
                    similarity = intersection / union
                    if similarity > 0.01:  # é–¾å€¤
                        similarity_edges.append((p1, p2, similarity))
                        
        return similarity_edges
    
    def metropolis_hastings_clustering(self, similarity_graph, n_iterations=1000):
        """
        Metropolis-Hastingsã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º
        ï¼ˆTwitterã®å®Ÿè£…ã‚’ç°¡ç•¥åŒ–ï¼‰
        """
        import random
        
        # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦
        nodes = list(set(n for edge in similarity_graph for n in edge[:2]))
        clusters = {node: random.randint(0, self.n_clusters-1) for node in nodes}
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¢æ•°ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£ï¼‰
        def calculate_energy(clusters, graph):
            energy = 0
            for n1, n2, weight in graph:
                if clusters[n1] == clusters[n2]:
                    energy += weight
            return energy
        
        current_energy = calculate_energy(clusters, similarity_graph)
        
        for _ in range(n_iterations):
            # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒ¼ãƒ‰ã‚’é¸æŠ
            node = random.choice(nodes)
            old_cluster = clusters[node]
            new_cluster = random.randint(0, self.n_clusters-1)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å¤‰æ›´
            clusters[node] = new_cluster
            new_energy = calculate_energy(clusters, similarity_graph)
            
            # MetropolisåŸºæº–
            delta = new_energy - current_energy
            if delta > 0 or random.random() < np.exp(delta):
                current_energy = new_energy
            else:
                clusters[node] = old_cluster  # å…ƒã«æˆ»ã™
                
        return clusters
    
    def create_embeddings(self, known_for, interested_in):
        """
        æœ€çµ‚çš„ãªåŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ
        """
        # Producer Embeddings: ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾
        # å˜ä¸€ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆKnownForï¼‰ã§ã¯ãªãã€è¤‡æ•°ã‚¯ãƒ©ã‚¹ã‚¿ã¸ã®æ‰€å±åº¦
        producer_embeddings = {}
        
        for producer, cluster in known_for.items():
            embedding = np.zeros(self.n_clusters)
            
            # ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ã‚¿
            embedding[cluster] = 1.0
            
            # é–¢é€£ã‚¯ãƒ©ã‚¹ã‚¿ã‚‚è€ƒæ…®ï¼ˆå®Ÿéš›ã¯ã‚‚ã£ã¨è¤‡é›‘ï¼‰
            # ...
            
            producer_embeddings[producer] = embedding
            
        return producer_embeddings
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ï¼ˆANNï¼‰

```python
from annoy import AnnoyIndex

class FastSimilaritySearch:
    """é«˜é€Ÿãªé¡ä¼¼ã‚¢ã‚¤ãƒ†ãƒ æ¤œç´¢"""
    
    def __init__(self, n_dimensions, n_trees=10):
        self.index = AnnoyIndex(n_dimensions, 'angular')
        self.n_trees = n_trees
        
    def build_index(self, item_vectors):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        for i, vector in enumerate(item_vectors):
            self.index.add_item(i, vector)
            
        self.index.build(self.n_trees)
        
    def find_similar(self, query_vector, n=10):
        """é¡ä¼¼ã‚¢ã‚¤ãƒ†ãƒ ã‚’é«˜é€Ÿæ¤œç´¢"""
        # O(log n)ã§æ¤œç´¢
        similar_items = self.index.get_nns_by_vector(
            query_vector, n, include_distances=True
        )
        return similar_items
```

### 2. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’

```python
class OnlineCollaborativeFiltering:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°å¯èƒ½ãªå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    
    def __init__(self, n_factors=50, learning_rate=0.01):
        self.n_factors = n_factors
        self.lr = learning_rate
        self.user_factors = {}
        self.item_factors = {}
        
    def update(self, user_id, item_id, rating):
        """æ–°ã—ã„è©•ä¾¡ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ï¼ˆSGDï¼‰"""
        # å› å­ã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°åˆæœŸåŒ–ï¼‰
        u_factors = self.user_factors.get(user_id, 
                                         np.random.normal(0, 0.1, self.n_factors))
        i_factors = self.item_factors.get(item_id,
                                         np.random.normal(0, 0.1, self.n_factors))
        
        # äºˆæ¸¬ã¨èª¤å·®
        prediction = np.dot(u_factors, i_factors)
        error = rating - prediction
        
        # SGDæ›´æ–°
        u_factors += self.lr * (error * i_factors - 0.01 * u_factors)
        i_factors += self.lr * (error * u_factors - 0.01 * i_factors)
        
        # ä¿å­˜
        self.user_factors[user_id] = u_factors
        self.item_factors[item_id] = i_factors
```

## ğŸ¯ å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’3-1: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
def exercise_hybrid_cf():
    """
    ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹
    """
    # TODO: å®Ÿè£…
    pass
```

### æ¼”ç¿’3-2: ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã¸ã®å¯¾å‡¦

```python
def exercise_cold_start():
    """
    æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ã¸ã®æ¨è–¦
    """
    # TODO: å®Ÿè£…
    pass
```

## ğŸ“ˆ è©•ä¾¡æŒ‡æ¨™

```python
def evaluate_cf_model(model, test_data):
    """å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®è©•ä¾¡"""
    
    # RMSEï¼ˆRoot Mean Square Errorï¼‰
    predictions = []
    actuals = []
    
    for user_id, item_id, rating in test_data:
        pred = model.predict(user_id, item_id)
        predictions.append(pred)
        actuals.append(rating)
        
    rmse = np.sqrt(np.mean((np.array(predictions) - np.array(actuals))**2))
    
    # MAPï¼ˆMean Average Precisionï¼‰
    # Precision@K
    # Recall@K
    # NDCGï¼ˆNormalized Discounted Cumulative Gainï¼‰
    
    return {
        'rmse': rmse,
        'map': map_score,
        'precision_at_10': precision,
        'recall_at_10': recall,
        'ndcg': ndcg_score
    }
```

## ğŸ’¡ ã¾ã¨ã‚

- **ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹**: ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆå¯èƒ½ã€ä½†ã—è¨ˆç®—é‡å¤§
- **ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹**: é«˜é€Ÿã§æ±åŒ–æ€§èƒ½é«˜ã„ã€ä½†ã—ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹
- **SimClusters**: ä¸¡è€…ã®è‰¯ã„ã¨ã“å–ã‚Šã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§è§£é‡ˆå¯èƒ½
- **æœ€é©åŒ–**: ANNã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã€åˆ†æ•£å‡¦ç†ãŒéµ

[â†’ Chapter 4: ç‰¹å¾´é‡ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¸](chapter04_features_ranking.md)